{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('D:\\\\Analytics\\\\Python ML\\\\Twitter Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Creating a Twitter App\n",
    "\n",
    "In order to extract tweets for a posterior analysis, we need to access to our Twitter account and create an app. The website to do this is https://apps.twitter.com/. (If you don't know how to do this, you can follow this tutorial video to create an account and an application.)\n",
    "\n",
    "From this app that we're creating we will save the following information in a script called credentials.py:\n",
    "\n",
    "Consumer Key (API Key)\n",
    "Consumer Secret (API Secret)\n",
    "Access Token\n",
    "Access Token Secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason of creating this extra file is that we want to export only the value of this variables, but being unseen in our main code (our notebook). We are now able to consume Twitter's API. In order to do this, we will create a function to allow us our keys authentication. We will add this function in another cell of code and we will run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import our access keys:\n",
    "from api_keys import * # This will allow us to use the keys as variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API's setup:\n",
    "def twitter_setup():\n",
    "    \"\"\"\n",
    "    Utility function to setup the Twitter's API\n",
    "    with our access keys provided.\n",
    "    \"\"\"\n",
    "    # Authentication and access using keys:\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "    # Return API with authentication:\n",
    "    api = tweepy.API(auth)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tweets extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created a function to setup the Twitter API, we can use this function to create an \"extractor\" object. After this, we will use Tweepy's function extractor.user_timeline(screen_name, count) to extract from screen_name's user the quantity of count tweets.\n",
    "\n",
    "As it is mentioned in the title, I've chosen @elonmusk as the user to extract data for a posterior analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an extractor object:\n",
    "extractor = twitter_setup()\n",
    "\n",
    "# We create a tweet list as follows:\n",
    "tweets = extractor.user_timeline(screen_name='IncomeTaxIndia', count=200) #Here the max-tweets, we can get are 200, because of this particular API\n",
    "print('No of tweets extracted: ',len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are printing the 1st 5 recent tweets + with attributes of every tweet\n",
    "for tweet in tweets[:5]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are printing the 1st 5 recent tweets + with their text as attributes of every tweet\n",
    "for tweet in tweets[:5]:\n",
    "    print(tweet.text,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now have an extractor and extracted data, which is listed in the tweets variable. I must mention at this point that each element in that list is a tweet object from Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type of tweets:\n",
    "type(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creating a (pandas) DataFrame\n",
    "\n",
    "We now have initial information to construct a pandas DataFrame, in order to manipulate the info in a very easy way.\n",
    "\n",
    "IPython's display function plots an output in a friendly way, and the headmethod of a dataframe allows us to visualize the first 5 elements of the dataframe (or the first number of elements that are passed as an argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We display the first 10 elements of the dataframe:\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal methods of a single tweet object:\n",
    "print(dir(tweets[0]))\n",
    "\n",
    "#print(dir(tweets)) - this will tell us the internal methods of a list as --tweets-- is list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The interesting part from here is the quantity of metadata contained in a single tweet. If we want to obtain data such as the creation date, or the source of creation, we can access the info with this attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Metadata of the recent Tweets, mean at zero index of tweets object:\\n')\n",
    "print('Tweet:',tweets[0].text)\n",
    "print('Id:',tweets[0].id)\n",
    "print('Created At:',tweets[0].created_at)\n",
    "print('Source:',tweets[0].source)\n",
    "print('Like:',tweets[0].favorite_count)\n",
    "print('Retweets:',tweets[0].retweet_count)\n",
    "print('Geo:',tweets[0].geo)\n",
    "print('Coordinates:',tweets[0].coordinates)\n",
    "print('Entities:',tweets[0].entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Adding relevant info to our dataframe\n",
    "\n",
    "As we can see, we can obtain a lot of data from a single tweet. But not all this data is always useful for specific stuff. In our case we well just add some data to our dataframe. For this we will use Pythons list comprehension and a new column will be added to the dataframe by just simply adding the name of the content between square brackets and assign the content. The code goes as...:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for deleting the columns and their samples of a Dataframe\n",
    "# del data['len'],data['ID'],data['Date'],data['Source'],data['Likes'],data['Retweets']\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding new columns in the dataframe\n",
    "# We add relevant data:\n",
    "data['Len'] = np.array([len(tweet.text) for tweet in tweets])\n",
    "data['ID'] = np.array([tweet.id for tweet in tweets])\n",
    "data['Date'] = np.array([tweet.created_at for tweet in tweets])\n",
    "data['Source'] = np.array([tweet.source for tweet in tweets])\n",
    "data['Likes'] = np.array([tweet.favorite_count for tweet in tweets])\n",
    "data['Retweets'] = np.array([tweet.retweet_count for tweet in tweets])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted and have the data in a easy-to-handle ordered way, we're ready to do a bit more of manipulation to visualize some plots and gather some statistical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Visualization and basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Averages and popularity\n",
    "\n",
    "We first want to calculate some basic statistical data, such as the mean of the length of characters of all tweets, the tweet with more likes and retweets, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(data['Len'])\n",
    "print(\"The lenght's average in tweets:\",mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We extract the tweet with more FAVs and more RTs:\n",
    "fav_max = np.max(data['Likes'])\n",
    "rt_max = np.max(data['Retweets'])\n",
    "\n",
    "#here, index[0] of dataframe i.e data is Tweets\n",
    "fav_tweet = data[data.Likes == fav_max].index[0]\n",
    "rt_tweet = data[data.Retweets == rt_max].index[0]\n",
    "\n",
    "# Max FAVs: + formatting\n",
    "print(\"The tweet with more likes is: \\n{}\".format(data['Tweets'][fav_tweet]))\n",
    "print(\"Number of likes: {}\".format(fav_max))\n",
    "print(\"{} characters.\\n\".format(data['Len'][fav_tweet]))\n",
    "\n",
    "# Max RTs: + formatting\n",
    "print(\"The tweet with more retweets is: \\n{}\".format(data['Tweets'][rt_tweet]))\n",
    "print(\"Number of retweets: {}\".format(rt_max))\n",
    "print(\"{} characters.\\n\".format(data['Len'][rt_tweet]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is common, but it won't necessarily happen: the tweet with more likes is the tweet with more retweets. What we're doing is that we find the maximum number of likes from the 'Likes' column and the maximum number of retweets from the 'RTs' using numpy's max function. With this we just look for the index in each of both columns that satisfy to be the maximum. Since more than one could have the same number of likes/retweets (the maximum) we just need to take the first one found, and that's why we use .index[0] to assign the index to the variables favand rt. To print the tweet that satisfies, we access the data in the same way we would access a matrix or any indexed object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Time series\n",
    "\n",
    "Pandas has its own object for time series. Since we have a whole vector with creation dates, we can construct time series respect tweets lengths, likes and retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create time series for data:\n",
    "tlen = pd.Series(data=data['Len'].values, index=data['Date'])\n",
    "tfav = pd.Series(data=data['Likes'].values, index=data['Date'])\n",
    "trt = pd.Series(data=data['Retweets'].values, index=data['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to plot the time series, pandas already has its own method in the object. We can plot a time series as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y-axis: Tweet's length & x-axis: Tweet's Date\n",
    "tlen.plot(figsize=(16,4), color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfav.plot(figsize=(16,4), label='Likes', legend=True)\n",
    "trt.plot(figsize=(16,4), label='Retweets', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Pie charts of sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost done with this second section of the post. Now we will plot the sources in a pie chart, since we realized that not every tweet is tweeted from the same source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We obtain all possible sources:\n",
    "sources = []\n",
    "for source in data['Source']:\n",
    "    if source not in sources:\n",
    "        sources.append(source)\n",
    "        \n",
    "# We print sources list:\n",
    "print('The distinct sources of tweets are:\\n')\n",
    "i=0\n",
    "for source in sources:\n",
    "    i+=1\n",
    "    print(str(i)+'. '+source)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now count the number of each source and create a pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a numpy vector mapped to labels:\n",
    "#Here, a percent 1D array is created which will store percentage of distincts tweets sources.\n",
    "#As we are providing len(sources), which will give a number and np.zeros() will create an array.\n",
    "percent = np.zeros(len(sources))\n",
    "percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in data['Source']:\n",
    "    for index in range(len(sources)):\n",
    "        if source == sources[index]:\n",
    "            percent[index] += 1\n",
    "            pass\n",
    "        \n",
    "percent /= 100\n",
    "percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie chart:\n",
    "#autopct enables you to display the percent value using Python string formatting. For example, if autopct='%.2f', \n",
    "#then for each pie wedge, the format string is '%.2f' \n",
    "#and the numerical percent value for that wedge is pct, so the wedge label is set to the string '%.2f'%pct\n",
    "pie_chart = pd.Series(percent, index=sources, name='Sources')\n",
    "pie_chart.plot.pie(fontsize=11, autopct='%.2f', figsize=(7,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the percentage of tweets per source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned at the beginning of this post, textblob will allow us to do sentiment analysis in a very simple way. We will also use the re library from Python, which is used to work with regular expressions. For this, I'll provide you two utility functions to: a) clean text (which means that any symbol distinct to an alphanumeric value will be remapped into a new one that satisfies this condition), and b) create a classifier to analyze the polarity of each tweet after cleaning the text in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) Removing Twitter Handles (@user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the tweets contain lots of twitter handles (@user), that is how a Twitter user acknowledged on Twitter. We will remove all these twitter handles from the data as they don’t convey much information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) Removing Punctuations, Numbers, and Special Characters\n",
    "\n",
    "As discussed, punctuations, numbers and special characters do not help much. It is better to remove them from the text just as we removed the twitter handles. Here we will replace everything except characters and hashtags with spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C) Others\n",
    "\n",
    "1. Removing Short Words - We have to be a little careful here in selecting the length of the words which we want to remove. So, I have decided to remove all the words having length 3 or less. For example, terms like “hmm”, “oh” are of very little use. It is better to get rid of them.\n",
    "\n",
    "Tokenization  - Now we will tokenize all the cleaned tweets in our dataset. Tokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.\n",
    "\n",
    "2. Stemming - Stemming is a rule-based process of stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word. For example, For example – “play”, “player”, “played”, “plays” and “playing” are the different variations of the word – “play”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Utility function to clean the text in a tweet by removing \n",
    "    links and special characters using regex.\n",
    "    '''\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(tweet):\n",
    "    '''\n",
    "    Utility function to classify the polarity of a tweet\n",
    "    using textblob.\n",
    "    '''\n",
    "    analysis = TextBlob(clean_tweet(tweet))\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 1 #+ve polarity => +ve characteristics\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0 #neutral polarity => neutral characteristics\n",
    "    else:\n",
    "        return -1 #-ve polarity => -ve characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way it works is that textblob already provides a trained analyzer (cool, right?). Textblob can work with different machine learning models used in natural language processing. If you want to train your own classifier (or at least check how it works) feel free to check the following link [https://textblob.readthedocs.io/en/dev/classifiers.html]. It might result relevant since we're working with a pre-trained model (for which we don't not the data that was used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sentiment-Analysis'] = np.array([analyze_sentiment(tweet) for tweet in data['Tweets']])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Analyzing the results\n",
    "\n",
    "To have a simple way to verify the results, we will count the number of neutral, positive and negative tweets and extract the percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We construct lists with classified tweets:\n",
    "pos_tweets = [tweet for index,tweet in enumerate(data['Tweets']) if data['Sentiment-Analysis'][index] > 0]\n",
    "neu_tweets = [tweet for index,tweet in enumerate(data['Tweets']) if data['Sentiment-Analysis'][index] == 0]\n",
    "neg_tweets = [tweet for index,tweet in enumerate(data['Tweets']) if data['Sentiment-Analysis'][index] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the lists, we just print the percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We print percentages:\n",
    "print('Percentage of Positive Tweets: ',int(len(pos_tweets))*100/int(len(data['Tweets'])))\n",
    "print('Percentage of Neutral Tweets: ',int(len(neu_tweets))*100/int(len(data['Tweets'])))\n",
    "print('Percentage of Negative Tweets: ',int(len(neg_tweets))*100/int(len(data['Tweets'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to consider that we're working only with the 200 most recent tweets from IncomeTaxIndia. For more accurate results we can consider more tweets. An interesting thing (an invitation to the readers) is to analyze the polarity of the tweets from different sources, it might be deterministic that by only considering the tweets from one source the polarity would result more positive/negative. Anyway, I hope this resulted interesting.\n",
    "\n",
    "As we saw, we can extract, manipulate, visualize and analyze data in a very simple way with Python. I hope that this leaves some uncertainty in the reader, for further exploration using this tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story Generation and Visualization from Tweets\n",
    "\n",
    "In this section, we will explore the cleaned tweets text. Exploring and visualizing data, no matter whether its text or any other data, is an essential step in gaining insights. Do not limit yourself to only these methods told in this tutorial, feel free to explore the data as much as possible.\n",
    "\n",
    "Before we begin exploration, we must think and ask questions related to the data in hand. A few probable questions are as follows:\n",
    "\n",
    "1. What are the most common words in the entire dataset?\n",
    "2. What are the most common words in the dataset for negative and positive tweets, respectively?\n",
    "3. How many hashtags are there in a tweet?\n",
    "4. Which trends are associated with my dataset?\n",
    "5. Which trends are associated with either of the sentiments? Are they compatible with the sentiments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([tweet for tweet in data['Tweets']])\n",
    "clean_all_words = clean_tweet(all_words)\n",
    "print(clean_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(clean_all_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Words in +ve sentiment tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = ' '.join([tweet for tweet in data['Tweets'][data['Sentiment-Analysis'] == 1]])\n",
    "clean_pos_words = clean_tweet(pos_words)\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(clean_pos_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Words in neutral sentiment tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neu_words = ' '.join([tweet for tweet in data['Tweets'][data['Sentiment-Analysis'] == 0]])\n",
    "clean_neu_words = clean_tweet(neu_words)\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(clean_neu_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Words in -ve sentiment tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words = ' '.join([tweet for tweet in data['Tweets'][data['Sentiment-Analysis'] == -1]])\n",
    "clean_neg_words = clean_tweet(pos_words)\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(clean_neg_words)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
